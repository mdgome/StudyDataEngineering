{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "76abd8c8-2af9-41f7-9ab0-80fde3fec8bd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import sys \n",
    "# !conda install -c conda-forge --prefix {sys.prefix} -y findspark\n",
    "# !conda install -c conda-forge --prefix {sys.prefix} -y requests\n",
    "# !conda install -c conda-forge --prefix {sys.prefix} -y json\n",
    "# !conda install -c conda-forge --prefix {sys.prefix} -y pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d27ff8ab-233a-4aa8-9816-7bac83af161f",
   "metadata": {},
   "source": [
    "### Findspark\n",
    "findspark 초기화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0b1380bf-53f8-48d1-90b7-bba9a7016aa8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import findspark\n",
    "# findspark.add_packages('mysql:mysql-connector-java-8.0.28')\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18e82d0d-fc5d-4093-bb01-f10f4a3a09f0",
   "metadata": {},
   "source": [
    "### PySpark 관련 패키지 import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ab1b1c45-6b9d-43cd-b100-f3a50509cf5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "# from pyspark.sql.types import ArrayType, IntegerType, StructType, StructField\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30433a39-90b4-4152-b699-c5078cdb90c6",
   "metadata": {},
   "source": [
    "### api 처리 패키지 import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6b3346fb-a04f-4dd7-a8de-120f5664ae55",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "import requests\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66aa9311-29dc-4c0f-ad23-f2c29f624d22",
   "metadata": {
    "tags": []
   },
   "source": [
    "### General 패키지 import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4daa5db4-1a49-40fe-8cea-8a05886de993",
   "metadata": {},
   "outputs": [],
   "source": [
    "import configparser\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "611c4c3d-618c-4068-9727-de850bb6b8ee",
   "metadata": {},
   "source": [
    "### Local SparkCluster connected session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "47c3f81a-70c4-4e9f-9897-bf6cb42dc3ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/03/09 01:23:49 WARN Utils: Your hostname, Jungminui-MacBookPro.local resolves to a loopback address: 127.0.0.1; using 172.30.1.19 instead (on interface en0)\n",
      "22/03/09 01:23:49 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "22/03/09 01:23:50 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .master('local') \\\n",
    "        .appName('tempera_obs') \\\n",
    "        .config(\"spark.driver.extraClassPath\", \"/Users/mdgome/Downloads/mysql-connector-java-8.0.28/mysql-connector-java-8.0.28.jar\") \\\n",
    "        .getOrCreate()\n",
    "#        .master('spark://hadoop01:7077') \\\n",
    "       # .config(\"spark.driver.extraClassPath\", \"/hadoop/jupyter_dir/jar/mysql-connector-java-8.0.28/mysql-connector-java-8.0.28.jar\") \\\n",
    "            # .config(\"spark.driver.extraClassPath\", \"hdfs:///spark3-jars/mysql-connector-java-8.0.28.jar\") \\"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5af24ea1-21b6-49df-b781-cc9086da7d42",
   "metadata": {},
   "source": [
    "#### DB Server Configure & API Service Key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1aa17fe1-18bb-4c66-835e-94e3ed6b401b",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = configparser.ConfigParser()\n",
    "config.read(Path(\"./config/config.ini\"),encoding='utf-8')\n",
    "# config.read(os.getcwd()+os.sep+'config'+os.sep+'config.ini',encoding='utf-8')\n",
    "\n",
    "user = config['dev_mysql']['user']\n",
    "password = config['dev_mysql']['password']\n",
    "host = config['dev_mysql']['host']\n",
    "port = config['dev_mysql']['port']\n",
    "dbname = config['dev_mysql']['dbname']\n",
    "url = config['dev_mysql']['url'].format(host=host,port=port,dbname=dbname)\n",
    "service_key=config['api']['service_key']\n",
    "# dbtable = config['dev_mysql']['dbtable']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d2fe942-1220-46da-ac14-6620cbd87d1e",
   "metadata": {},
   "source": [
    "#### 관측소 정보 table 불러옴"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3d27af11-c196-472b-a6ee-9b4a835767b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading class `com.mysql.jdbc.Driver'. This is deprecated. The new driver class is `com.mysql.cj.jdbc.Driver'. The driver is automatically registered via the SPI and manual loading of the driver class is generally unnecessary.\n",
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+----------+----------------------------------+-----------+-------------+\n",
      "| data_type|  obs_lat|   obs_lon|                        obs_object|obs_post_id|obs_post_name|\n",
      "+----------+---------+----------+----------------------------------+-----------+-------------+\n",
      "|조위관측소|35.024178|128.810933|조위,수온,염분,기온,기압,풍속,풍향|    DT_0063|       가덕도|\n",
      "|조위관측소|37.731944|126.522222|                         조위,기압|    DT_0032|     강화대교|\n",
      "|조위관측소|34.028333|127.308889|조위,수온,염분,기온,기압,풍속,풍향|    DT_0031|       거문도|\n",
      "|조위관측소|34.801389|128.699167|조위,수온,염분,기온,기압,풍속,풍향|    DT_0029|       거제도|\n",
      "|조위관측소|37.560833|126.601111|          조위,기온,기압,풍속,풍향|    DT_0058|       경인항|\n",
      "+----------+---------+----------+----------------------------------+-----------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# obs_post_data = pd.read_csv(\"./관측소 정보.csv\")\n",
    "obs_post_data= spark.read.format('jdbc').options(\n",
    "    url=url,\n",
    "    driver='com.mysql.jdbc.Driver',\n",
    "    dbtable='obs_info',\n",
    "    user=user,\n",
    "    password=password).load()\n",
    "obs_post_data.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec1db621-7134-452a-a7ee-d1b633511fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_df = obs_post_data.select(split(col(\"obs_object\"), \",\").alias(\"obs_object_array\")).drop(\"obs_object\")\n",
    "# obs_post_data.createOrReplaceTempView(\"obs_post\")\n",
    "# obs_post_data = spark.sql(\"select *, SPLIT(obs_object,',') as obs_object_array from obs_post order\").drop(\"obs_object\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f8a4b9e-d174-4db6-8bdf-c4ef7edd79ea",
   "metadata": {},
   "source": [
    "## 관측소 별 호출 가능한 API 정리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6fdadb9e-978f-4041-802c-13d1c6dce874",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+---------+----------+----------------------------+------------+\n",
      "|obs_post_id|obs_post_name|  obs_lat|   obs_lon|            obs_object_array|   data_type|\n",
      "+-----------+-------------+---------+----------+----------------------------+------------+\n",
      "|    DT_0065|       덕적도|37.226333|126.156556|    [기온, 기압, 풍속, 풍향]|  조위관측소|\n",
      "|    HB_0002|  한수원_고리|  35.3185|129.314722|[수온, 기온, 기압, 풍속, ...|해양관측부이|\n",
      "|    HB_0008|  한수원_덕천|     37.1|129.404167|[수온, 기온, 기압, 풍속, ...|해양관측부이|\n",
      "|    TW_0079|     상왕등도|35.652472| 126.19425|[수온, 기온, 기압, 풍속, ...|해양관측부이|\n",
      "|    KG_0024|     대한해협|   34.919| 129.12125|[수온, 기온, 기압, 풍속, ...|해양관측부이|\n",
      "+-----------+-------------+---------+----------+----------------------------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "obs_info_df = obs_post_data.select(obs_post_data.obs_post_id,obs_post_data.obs_post_name,obs_post_data.obs_lat,obs_post_data.obs_lon,split(col(\"obs_object\"), \",\").alias(\"obs_object_array\"),obs_post_data.data_type).drop(\"obs_object\")\n",
    "obs_info_df = obs_info_df.sort(col(\"obs_object_array\"),col(\"obs_post_id\"))\n",
    "obs_info_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6c97f7c4-ffde-442d-8d45-595149155ced",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DT_0065 ['기온', '기압', '풍속', '풍향'] 조위관측소\n",
      "HB_0002 ['수온', '기온', '기압', '풍속', '풍향', '유속', '유향'] 해양관측부이\n",
      "HB_0008 ['수온', '기온', '기압', '풍속', '풍향', '유속', '유향'] 해양관측부이\n",
      "TW_0079 ['수온', '기온', '기압', '풍속', '풍향', '유속', '유향'] 해양관측부이\n",
      "KG_0024 ['수온', '기온', '기압', '풍속', '풍향', '파고', '유속', '유향'] 해양관측부이\n",
      "KG_0025 ['수온', '기온', '기압', '풍속', '풍향', '파고', '유속', '유향'] 해양관측부이\n",
      "KG_0101 ['수온', '기온', '기압', '풍속', '풍향', '파고', '유속', '유향'] 해양관측부이\n",
      "TW_0062 ['수온', '기온', '기압', '풍속', '풍향', '파고', '유속', '유향'] 해양관측부이\n",
      "TW_0069 ['수온', '기온', '기압', '풍속', '풍향', '파고', '유속', '유향'] 해양관측부이\n",
      "TW_0075 ['수온', '기온', '기압', '풍속', '풍향', '파고', '유속', '유향'] 해양관측부이\n",
      "TW_0080 ['수온', '기온', '기압', '풍속', '풍향', '파고', '유속', '유향'] 해양관측부이\n",
      "TW_0081 ['수온', '기온', '기압', '풍속', '풍향', '파고', '유속', '유향'] 해양관측부이\n",
      "TW_0089 ['수온', '기온', '기압', '풍속', '풍향', '파고', '유속', '유향'] 해양관측부이\n",
      "TW_0090 ['수온', '기온', '기압', '풍속', '풍향', '파고', '유속', '유향'] 해양관측부이\n",
      "TW_0091 ['수온', '기온', '기압', '풍속', '풍향', '파고', '유속', '유향'] 해양관측부이\n",
      "TW_0092 ['수온', '기온', '기압', '풍속', '풍향', '파고', '유속', '유향'] 해양관측부이\n",
      "TW_0093 ['수온', '기온', '기압', '풍속', '풍향', '파고', '유속', '유향'] 해양관측부이\n",
      "TW_0094 ['수온', '기온', '기압', '풍속', '풍향', '파고', '유속', '유향'] 해양관측부이\n",
      "HB_0001 ['수온', '유속', '유향'] 해양관측부이\n",
      "HB_0003 ['수온', '유속', '유향'] 해양관측부이\n",
      "HB_0007 ['수온', '유속', '유향'] 해양관측부이\n",
      "HB_0009 ['수온', '유속', '유향'] 해양관측부이\n",
      "KG_0021 ['수온', '유속', '유향'] 해양관측부이\n",
      "SF_0001 ['수온', '유속', '유향'] 해양관측부이\n",
      "SF_0005 ['수온', '유속', '유향'] 해양관측부이\n",
      "TW_0070 ['수온', '유속', '유향'] 해양관측부이\n",
      "TW_0072 ['수온', '유속', '유향'] 해양관측부이\n",
      "TW_0074 ['수온', '유속', '유향'] 해양관측부이\n",
      "TW_0076 ['수온', '유속', '유향'] 해양관측부이\n",
      "TW_0077 ['수온', '유속', '유향'] 해양관측부이\n",
      "TW_0078 ['수온', '유속', '유향'] 해양관측부이\n",
      "TW_0082 ['수온', '유속', '유향'] 해양관측부이\n",
      "TW_0083 ['수온', '유속', '유향'] 해양관측부이\n",
      "TW_0084 ['수온', '유속', '유향'] 해양관측부이\n",
      "TW_0085 ['수온', '유속', '유향'] 해양관측부이\n",
      "TW_0086 ['수온', '유속', '유향'] 해양관측부이\n",
      "TW_0087 ['수온', '유속', '유향'] 해양관측부이\n",
      "TW_0088 ['수온', '유속', '유향'] 해양관측부이\n",
      "DT_0039 ['실시간제공데이터 항목 없음'] 해양관측소\n",
      "KG_0028 ['실시간제공데이터 항목 없음'] 해양관측부이\n",
      "KG_0102 ['실시간제공데이터 항목 없음'] 해양관측부이\n",
      "DT_0032 ['조위', '기압'] 조위관측소\n",
      "DT_0044 ['조위', '기압'] 조위관측소\n",
      "DT_0003 ['조위', '기온', '기압', '풍속', '풍향'] 조위관측소\n",
      "DT_0024 ['조위', '기온', '기압', '풍속', '풍향'] 조위관측소\n",
      "DT_0043 ['조위', '기온', '기압', '풍속', '풍향'] 조위관측소\n",
      "DT_0049 ['조위', '기온', '기압', '풍속', '풍향'] 조위관측소\n",
      "DT_0050 ['조위', '기온', '기압', '풍속', '풍향'] 조위관측소\n",
      "DT_0051 ['조위', '기온', '기압', '풍속', '풍향'] 조위관측소\n",
      "DT_0052 ['조위', '기온', '기압', '풍속', '풍향'] 조위관측소\n",
      "DT_0056 ['조위', '기온', '기압', '풍속', '풍향'] 조위관측소\n",
      "DT_0057 ['조위', '기온', '기압', '풍속', '풍향'] 조위관측소\n",
      "DT_0058 ['조위', '기온', '기압', '풍속', '풍향'] 조위관측소\n",
      "DT_0066 ['조위', '기온', '기압', '풍속', '풍향'] 조위관측소\n",
      "DT_0068 ['조위', '기온', '기압', '풍속', '풍향'] 조위관측소\n",
      "DT_0092 ['조위', '기온', '기압', '풍속', '풍향'] 조위관측소\n",
      "DT_0041 ['조위', '기온', '기압', '풍속', '풍향', '파고'] 해양관측소\n",
      "DT_0042 ['조위', '기온', '기압', '풍속', '풍향', '파고'] 해양관측소\n",
      "IE_0061 ['조위', '기온', '기압', '풍속', '풍향', '파고'] 해양과학기지\n",
      "DT_0002 ['조위', '수온', '기온', '기압', '풍속', '풍향'] 조위관측소\n",
      "DT_0017 ['조위', '수온', '기온', '기압', '풍속', '풍향'] 조위관측소\n",
      "IE_0062 ['조위', '수온', '기온', '기압', '풍속', '풍향', '파고'] 해양과학기지\n",
      "DT_0001 ['조위', '수온', '염분', '기온', '기압', '풍속', '풍향'] 조위관측소\n",
      "DT_0004 ['조위', '수온', '염분', '기온', '기압', '풍속', '풍향'] 조위관측소\n",
      "DT_0005 ['조위', '수온', '염분', '기온', '기압', '풍속', '풍향'] 조위관측소\n",
      "DT_0006 ['조위', '수온', '염분', '기온', '기압', '풍속', '풍향'] 조위관측소\n",
      "DT_0007 ['조위', '수온', '염분', '기온', '기압', '풍속', '풍향'] 조위관측소\n",
      "DT_0008 ['조위', '수온', '염분', '기온', '기압', '풍속', '풍향'] 조위관측소\n",
      "DT_0010 ['조위', '수온', '염분', '기온', '기압', '풍속', '풍향'] 조위관측소\n",
      "DT_0011 ['조위', '수온', '염분', '기온', '기압', '풍속', '풍향'] 조위관측소\n",
      "DT_0012 ['조위', '수온', '염분', '기온', '기압', '풍속', '풍향'] 조위관측소\n",
      "DT_0013 ['조위', '수온', '염분', '기온', '기압', '풍속', '풍향'] 조위관측소\n",
      "DT_0014 ['조위', '수온', '염분', '기온', '기압', '풍속', '풍향'] 조위관측소\n",
      "DT_0016 ['조위', '수온', '염분', '기온', '기압', '풍속', '풍향'] 조위관측소\n",
      "DT_0018 ['조위', '수온', '염분', '기온', '기압', '풍속', '풍향'] 조위관측소\n",
      "DT_0020 ['조위', '수온', '염분', '기온', '기압', '풍속', '풍향'] 조위관측소\n",
      "DT_0021 ['조위', '수온', '염분', '기온', '기압', '풍속', '풍향'] 조위관측소\n",
      "DT_0022 ['조위', '수온', '염분', '기온', '기압', '풍속', '풍향'] 조위관측소\n",
      "DT_0023 ['조위', '수온', '염분', '기온', '기압', '풍속', '풍향'] 조위관측소\n",
      "DT_0025 ['조위', '수온', '염분', '기온', '기압', '풍속', '풍향'] 조위관측소\n",
      "DT_0026 ['조위', '수온', '염분', '기온', '기압', '풍속', '풍향'] 조위관측소\n",
      "DT_0027 ['조위', '수온', '염분', '기온', '기압', '풍속', '풍향'] 조위관측소\n",
      "DT_0028 ['조위', '수온', '염분', '기온', '기압', '풍속', '풍향'] 조위관측소\n",
      "DT_0029 ['조위', '수온', '염분', '기온', '기압', '풍속', '풍향'] 조위관측소\n",
      "DT_0031 ['조위', '수온', '염분', '기온', '기압', '풍속', '풍향'] 조위관측소\n",
      "DT_0035 ['조위', '수온', '염분', '기온', '기압', '풍속', '풍향'] 조위관측소\n",
      "DT_0037 ['조위', '수온', '염분', '기온', '기압', '풍속', '풍향'] 조위관측소\n",
      "DT_0061 ['조위', '수온', '염분', '기온', '기압', '풍속', '풍향'] 조위관측소\n",
      "DT_0062 ['조위', '수온', '염분', '기온', '기압', '풍속', '풍향'] 조위관측소\n",
      "DT_0063 ['조위', '수온', '염분', '기온', '기압', '풍속', '풍향'] 조위관측소\n",
      "DT_0067 ['조위', '수온', '염분', '기온', '기압', '풍속', '풍향'] 조위관측소\n",
      "DT_0091 ['조위', '수온', '염분', '기온', '기압', '풍속', '풍향'] 조위관측소\n",
      "IE_0060 ['조위', '수온', '염분', '기온', '기압', '풍속', '풍향', '파고'] 해양과학기지\n"
     ]
    }
   ],
   "source": [
    "for row in obs_info_df.rdd.collect():\n",
    "    print(row['obs_post_id'], row['obs_object_array'], row['data_type'])\n",
    "    # print(type(row['obs_object']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "988e7e3c-402a-4bad-bd2f-5f16709abd0a",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-03 08:03:56,620 WARN scheduler.TaskSetManager: Lost task 13.0 in stage 8.0 (TID 22) (hadoop02 executor 1): java.io.IOException: Cannot run program \"/hadoop/anaconda3/envs/jungmin/bin/python\": error=2, 그런 파일이나 디렉터리가 없습니다\n",
      "\tat java.lang.ProcessBuilder.start(ProcessBuilder.java:1048)\n",
      "\tat org.apache.spark.api.python.PythonWorkerFactory.startDaemon(PythonWorkerFactory.scala:209)\n",
      "\tat org.apache.spark.api.python.PythonWorkerFactory.createThroughDaemon(PythonWorkerFactory.scala:132)\n",
      "\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:105)\n",
      "\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:119)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:145)\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: java.io.IOException: error=2, 그런 파일이나 디렉터리가 없습니다\n",
      "\tat java.lang.UNIXProcess.forkAndExec(Native Method)\n",
      "\tat java.lang.UNIXProcess.<init>(UNIXProcess.java:247)\n",
      "\tat java.lang.ProcessImpl.start(ProcessImpl.java:134)\n",
      "\tat java.lang.ProcessBuilder.start(ProcessBuilder.java:1029)\n",
      "\t... 16 more\n",
      "\n",
      "2022-03-03 08:03:56,838 ERROR scheduler.TaskSetManager: Task 13 in stage 8.0 failed 4 times; aborting job\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 13 in stage 8.0 failed 4 times, most recent failure: Lost task 13.3 in stage 8.0 (TID 26) (hadoop02 executor 1): java.io.IOException: Cannot run program \"/hadoop/anaconda3/envs/jungmin/bin/python\": error=2, 그런 파일이나 디렉터리가 없습니다\n\tat java.lang.ProcessBuilder.start(ProcessBuilder.java:1048)\n\tat org.apache.spark.api.python.PythonWorkerFactory.startDaemon(PythonWorkerFactory.scala:209)\n\tat org.apache.spark.api.python.PythonWorkerFactory.createThroughDaemon(PythonWorkerFactory.scala:132)\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:105)\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:119)\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:145)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: java.io.IOException: error=2, 그런 파일이나 디렉터리가 없습니다\n\tat java.lang.UNIXProcess.forkAndExec(Native Method)\n\tat java.lang.UNIXProcess.<init>(UNIXProcess.java:247)\n\tat java.lang.ProcessImpl.start(ProcessImpl.java:134)\n\tat java.lang.ProcessBuilder.start(ProcessBuilder.java:1029)\n\t... 16 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2258)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2207)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2206)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2206)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1079)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1079)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1079)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2445)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2387)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2376)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:868)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2196)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2217)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2236)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2261)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1030)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1029)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:180)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: java.io.IOException: Cannot run program \"/hadoop/anaconda3/envs/jungmin/bin/python\": error=2, 그런 파일이나 디렉터리가 없습니다\n\tat java.lang.ProcessBuilder.start(ProcessBuilder.java:1048)\n\tat org.apache.spark.api.python.PythonWorkerFactory.startDaemon(PythonWorkerFactory.scala:209)\n\tat org.apache.spark.api.python.PythonWorkerFactory.createThroughDaemon(PythonWorkerFactory.scala:132)\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:105)\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:119)\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:145)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\nCaused by: java.io.IOException: error=2, 그런 파일이나 디렉터리가 없습니다\n\tat java.lang.UNIXProcess.forkAndExec(Native Method)\n\tat java.lang.UNIXProcess.<init>(UNIXProcess.java:247)\n\tat java.lang.ProcessImpl.start(ProcessImpl.java:134)\n\tat java.lang.ProcessBuilder.start(ProcessBuilder.java:1029)\n\t... 16 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Input \u001b[0;32mIn [14]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0m obs_post_list \u001b[38;5;241m=\u001b[39m \u001b[43mobs_post_data\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mobs_post_id\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrdd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflatMap\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/spark-3.1.2/python/pyspark/rdd.py:949\u001b[0m, in \u001b[0;36mRDD.collect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    940\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    941\u001b[0m \u001b[38;5;124;03mReturn a list that contains all of the elements in this RDD.\u001b[39;00m\n\u001b[1;32m    942\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    946\u001b[0m \u001b[38;5;124;03mto be small, as all the data is loaded into the driver's memory.\u001b[39;00m\n\u001b[1;32m    947\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    948\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m SCCallSiteSync(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontext) \u001b[38;5;28;01mas\u001b[39;00m css:\n\u001b[0;32m--> 949\u001b[0m     sock_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jvm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPythonRDD\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollectAndServe\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jrdd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrdd\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    950\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(_load_from_socket(sock_info, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jrdd_deserializer))\n",
      "File \u001b[0;32m~/spark-3.1.2/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py:1304\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1298\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1299\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1300\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1301\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1303\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1304\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1305\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1307\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1308\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[0;32m~/spark-3.1.2/python/pyspark/sql/utils.py:111\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw):\n\u001b[1;32m    110\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 111\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    112\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m py4j\u001b[38;5;241m.\u001b[39mprotocol\u001b[38;5;241m.\u001b[39mPy4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    113\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m~/spark-3.1.2/python/lib/py4j-0.10.9-src.zip/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 13 in stage 8.0 failed 4 times, most recent failure: Lost task 13.3 in stage 8.0 (TID 26) (hadoop02 executor 1): java.io.IOException: Cannot run program \"/hadoop/anaconda3/envs/jungmin/bin/python\": error=2, 그런 파일이나 디렉터리가 없습니다\n\tat java.lang.ProcessBuilder.start(ProcessBuilder.java:1048)\n\tat org.apache.spark.api.python.PythonWorkerFactory.startDaemon(PythonWorkerFactory.scala:209)\n\tat org.apache.spark.api.python.PythonWorkerFactory.createThroughDaemon(PythonWorkerFactory.scala:132)\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:105)\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:119)\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:145)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: java.io.IOException: error=2, 그런 파일이나 디렉터리가 없습니다\n\tat java.lang.UNIXProcess.forkAndExec(Native Method)\n\tat java.lang.UNIXProcess.<init>(UNIXProcess.java:247)\n\tat java.lang.ProcessImpl.start(ProcessImpl.java:134)\n\tat java.lang.ProcessBuilder.start(ProcessBuilder.java:1029)\n\t... 16 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2258)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2207)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2206)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2206)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1079)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1079)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1079)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2445)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2387)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2376)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:868)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2196)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2217)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2236)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2261)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1030)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1029)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:180)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: java.io.IOException: Cannot run program \"/hadoop/anaconda3/envs/jungmin/bin/python\": error=2, 그런 파일이나 디렉터리가 없습니다\n\tat java.lang.ProcessBuilder.start(ProcessBuilder.java:1048)\n\tat org.apache.spark.api.python.PythonWorkerFactory.startDaemon(PythonWorkerFactory.scala:209)\n\tat org.apache.spark.api.python.PythonWorkerFactory.createThroughDaemon(PythonWorkerFactory.scala:132)\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:105)\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:119)\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:145)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\nCaused by: java.io.IOException: error=2, 그런 파일이나 디렉터리가 없습니다\n\tat java.lang.UNIXProcess.forkAndExec(Native Method)\n\tat java.lang.UNIXProcess.<init>(UNIXProcess.java:247)\n\tat java.lang.ProcessImpl.start(ProcessImpl.java:134)\n\tat java.lang.ProcessBuilder.start(ProcessBuilder.java:1029)\n\t... 16 more\n"
     ]
    }
   ],
   "source": [
    "obs_post_list = obs_post_data.select('obs_post_id').rdd.flatMap(lambda x: x).collect()\n",
    "# print(list(obs_post_list)) # 디버깅 용"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f84fc80-d7a4-4e27-9500-7dd67e93dd09",
   "metadata": {},
   "source": [
    "#### 전날 데이터 불러오기 위한 관련 코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "b2d8bf4f-492d-4034-b62e-1cadc665e6ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'20220301'"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yesterday = datetime.today() - timedelta(days = 1 )\n",
    "yesterday = yesterday.strftime('%Y%m%d')\n",
    "yesterday"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "95858571-209d-4d0d-b629-d3ef7d1057b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 기온 api\n",
    "url=\"http://www.khoa.go.kr/api/oceangrid/tideObsAirTemp/search.do?ServiceKey=\"+service_key+\"&ObsCode=DT_0004&Date=\"+yesterday+\"&ResultType=json\"\n",
    "# 기압 api\n",
    "url1=\"http://www.khoa.go.kr/api/oceangrid/tideObsAirPres/search.do?ServiceKey=\"+service_key+\"&ObsCode=DT_0004&Date=\"+yesterday+\"&ResultType=json\"\n",
    "# 풍속 api\n",
    "url2=\"http://www.khoa.go.kr/api/oceangrid/tideObsWind/search.do?ServiceKey=\"+service_key+\"&ObsCode=DT_0004&Date=\"+yesterday+\"&ResultType=json\"\n",
    "# 염분 api\n",
    "url3=\"http://www.khoa.go.kr/api/oceangrid/tideObsSalt/search.do?ServiceKey=\"+service_key+\"&ObsCode=DT_0004&Date=\"+yesterday+\"&ResultType=json\"\n",
    "# 조위 실측/예측 api\n",
    "url4=\"http://www.khoa.go.kr/api/oceangrid/tideCurPre/search.do?ServiceKey=\"+service_key+\"&ObsCode=DT_0004&Date=\"+yesterday+\"&ResultType=json\"\n",
    "# 수온 api\n",
    "url5=\"http://www.khoa.go.kr/api/oceangrid/tideObsTemp/search.do?ServiceKey=\"+service_key+\"&ObsCode=DT_0004&Date=\"+yesterday+\"&ResultType=json\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d06781d-9b26-4de6-ba6e-06c8be22b396",
   "metadata": {},
   "source": [
    "#### API data 가져옴"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "47faa936-f978-492f-aec1-3c84edbc7afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.get(url).json()\n",
    "response1 = requests.get(url1).json()\n",
    "response2 = requests.get(url2).json()\n",
    "response3 = requests.get(url3).json()\n",
    "response4 = requests.get(url4).json()\n",
    "response5 = requests.get(url5).json()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8b16edf-ae54-4133-999b-fc93af2b6c70",
   "metadata": {},
   "source": [
    "#### 가져온 data spark dataframe 으로 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "cbe29b37-21df-494b-a612-2fe68e9bb46c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.createDataFrame(response[\"result\"][\"data\"])\n",
    "df1 = spark.createDataFrame(response1[\"result\"][\"data\"])\n",
    "df2 = spark.createDataFrame(response2[\"result\"][\"data\"])\n",
    "df3 = spark.createDataFrame(response3[\"result\"][\"data\"])\n",
    "df4 = spark.createDataFrame(response4[\"result\"][\"data\"])\n",
    "df5 = spark.createDataFrame(response5[\"result\"][\"data\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98d56506-7b6e-4da8-83b1-bd356417c90c",
   "metadata": {},
   "source": [
    "#### df1, df2, df3, df5 하나의 dataframe 으로 합침"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "36c5858f-25de-408b-92a7-c2a498e338ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_df = df.join(df1,'record_time')#.select(df1.record_time, df1.air_pres, df.air_temp)\n",
    "output_df = output_df.join(df2, 'record_time')#.select(output_df.record_time,output_df.air_pres,output_df.air_temp,df2.wind_dir,df2.wind_speed)\n",
    "output_df = output_df.join(df3, 'record_time')#.select(output_df.record_time,output_df.air_pres,output_df.air_temp,output_df.wind_dir,output_df.wind_speed,df3.salinity)\n",
    "output_df = output_df.join(df5, 'record_time')#.select(output_df.record_time,output_df.air_pres,output_df.air_temp,output_df.wind_dir,output_df.wind_speed,df3.salinity)\n",
    "# output_df = output_df.join(df4, 'record_time')#.select(output_df.record_time,output_df.air_pres,output_df.air_temp,output_df.wind_dir,output_df.wind_speed,output_df.salinity,df4.pre_value,df4.real_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "767a2866-66ee-4cbb-88d4-179e58fe41ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- record_time: string (nullable = true)\n",
      " |-- air_temp: string (nullable = true)\n",
      " |-- air_pres: string (nullable = true)\n",
      " |-- wind_dir: string (nullable = true)\n",
      " |-- wind_speed: string (nullable = true)\n",
      " |-- salinity: string (nullable = true)\n",
      " |-- water_temp: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "output_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "3fab6c6e-5ea4-4198-8998-52d249cde43b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------+--------+--------+----------+--------+----------+\n",
      "|        record_time|air_temp|air_pres|wind_dir|wind_speed|salinity|water_temp|\n",
      "+-------------------+--------+--------+--------+----------+--------+----------+\n",
      "|2022-03-01 05:45:00|    13.6|  1013.4|   236.0|       5.4|    32.6|      13.3|\n",
      "|2022-03-01 07:27:00|    12.7|  1014.6|   298.0|       7.4|    32.5|      13.3|\n",
      "|2022-03-01 09:29:00|    11.5|  1014.9|   286.0|       5.4|    32.5|      13.4|\n",
      "|2022-03-01 12:58:00|    10.6|  1014.7|   282.0|       4.5|    32.5|      13.3|\n",
      "|2022-03-01 13:47:00|    11.3|  1014.2|   293.0|       5.0|    32.5|      13.2|\n",
      "+-------------------+--------+--------+--------+----------+--------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "output_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d69136ee-1a96-4f88-a14d-8b036aa0a8cd",
   "metadata": {},
   "source": [
    "#### 모든 data type 이 string으로 되어 있어 data type 변경"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "f512a4ef-4597-4606-ae96-b438365e8bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_df = output_df.withColumn('air_temp', output_df['air_temp'].cast(DoubleType()))\\\n",
    "    .withColumn('air_pres', output_df['air_pres'].cast(DoubleType()))\\\n",
    "    .withColumn('wind_dir', output_df['wind_dir'].cast(DoubleType()))\\\n",
    "    .withColumn('record_time', output_df['record_time'].cast(\"timestamp\"))\\\n",
    "    .withColumn('salinity', output_df['salinity'].cast(DoubleType()))\n",
    "    # .withColumn('pre_value', df4['pre_value'].cast(IntegerType())) \\\n",
    "    # .withColumn('real_value', df4['real_value'].cast(IntegerType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "2d6e271d-0cec-4537-b361-613daee62f7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- record_time: timestamp (nullable = true)\n",
      " |-- air_temp: double (nullable = true)\n",
      " |-- air_pres: double (nullable = true)\n",
      " |-- wind_dir: double (nullable = true)\n",
      " |-- wind_speed: string (nullable = true)\n",
      " |-- salinity: double (nullable = true)\n",
      " |-- water_temp: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "output_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "4356a820-d334-4fff-9615-42ced5f07ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "df4 = df4.withColumn('pre_value', df4['pre_value'].cast(IntegerType())) \\\n",
    "    .withColumn('record_time', df4['record_time'].cast(\"timestamp\"))\\\n",
    "    .withColumn('real_value', df4['real_value'].cast(IntegerType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "a19e8620-a34b-4320-a632-2b77a896499d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- pre_value: integer (nullable = true)\n",
      " |-- real_value: integer (nullable = true)\n",
      " |-- record_time: timestamp (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df4.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "5990b916-b215-4208-a99e-c4abf62738e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+-------------------+\n",
      "|pre_value|real_value|        record_time|\n",
      "+---------+----------+-------------------+\n",
      "|      134|       160|2022-03-01 00:00:00|\n",
      "|       92|       121|2022-03-01 01:00:00|\n",
      "|       53|        81|2022-03-01 02:00:00|\n",
      "|       23|        51|2022-03-01 03:00:00|\n",
      "|       14|        39|2022-03-01 04:00:00|\n",
      "+---------+----------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df4.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "dd42da7b-0db4-454d-8963-8fe7a244edf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------+--------+--------+----------+--------+----------+\n",
      "|        record_time|air_temp|air_pres|wind_dir|wind_speed|salinity|water_temp|\n",
      "+-------------------+--------+--------+--------+----------+--------+----------+\n",
      "|2022-03-01 00:00:00|    10.9|  1016.1|   278.0|       0.3|    32.6|      13.2|\n",
      "|2022-03-01 00:01:00|    10.9|  1016.0|   278.0|       0.4|    32.6|      13.2|\n",
      "|2022-03-01 00:02:00|    10.8|  1016.0|   278.0|       0.7|    32.6|      13.2|\n",
      "|2022-03-01 00:03:00|    10.7|  1016.0|   210.0|       1.0|    32.6|      13.2|\n",
      "|2022-03-01 00:04:00|    10.7|  1016.1|   160.0|       0.5|    32.6|      13.2|\n",
      "+-------------------+--------+--------+--------+----------+--------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "output_df = output_df.sort(asc(\"record_time\"))\n",
    "output_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "4ccae954-c3f5-43d9-8fc6-29be0564133d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------+--------+--------+----------+--------+----------+---------+----------+\n",
      "|        record_time|air_temp|air_pres|wind_dir|wind_speed|salinity|water_temp|pre_value|real_value|\n",
      "+-------------------+--------+--------+--------+----------+--------+----------+---------+----------+\n",
      "|2022-03-01 00:00:00|    10.9|  1016.1|   278.0|       0.3|    32.6|      13.2|     null|      null|\n",
      "|2022-03-01 00:00:00|    null|    null|    null|      null|    null|      null|      134|       160|\n",
      "|2022-03-01 00:01:00|    10.9|  1016.0|   278.0|       0.4|    32.6|      13.2|     null|      null|\n",
      "|2022-03-01 00:02:00|    10.8|  1016.0|   278.0|       0.7|    32.6|      13.2|     null|      null|\n",
      "|2022-03-01 00:03:00|    10.7|  1016.0|   210.0|       1.0|    32.6|      13.2|     null|      null|\n",
      "+-------------------+--------+--------+--------+----------+--------+----------+---------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "union_df = output_df.unionByName(df4, allowMissingColumns=True)\n",
    "union_df = union_df.sort(asc(\"record_time\"))\n",
    "union_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "cf9fd80f-26c1-456f-bdaa-b8a6a45305c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------+--------+--------+----------+--------+---------+----------+\n",
      "|        record_time|air_pres|air_temp|wind_dir|wind_speed|salinity|pre_value|real_value|\n",
      "+-------------------+--------+--------+--------+----------+--------+---------+----------+\n",
      "|2022-03-01 17:00:00|  1016.0|     9.5|   312.0|       5.5|    32.7|       60|        70|\n",
      "|2022-03-01 01:00:00|  1015.6|    11.2|   117.0|       1.7|    32.7|       92|       121|\n",
      "|2022-03-01 04:00:00|  1013.0|    12.2|   276.0|       1.3|    32.6|       14|        39|\n",
      "|2022-03-01 18:00:00|  1017.0|     8.4|   321.0|       5.6|    32.7|       84|        90|\n",
      "|2022-03-01 22:00:00|  1019.8|     7.4|   284.0|       4.2|    32.6|      214|       222|\n",
      "+-------------------+--------+--------+--------+----------+--------+---------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "output_df = output_df.join(df4, 'record_time').select(output_df.record_time,output_df.air_pres,output_df.air_temp,output_df.wind_dir,output_df.wind_speed,output_df.salinity,df4.pre_value,df4.real_value)\n",
    "output_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff6b0c21-7731-4217-9101-44591c5db629",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### 합쳐진 dataframe sorting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ce97e496-8c58-4174-b752-7230f1d5ad5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------+--------+--------+----------+--------+---------+----------+\n",
      "|        record_time|air_pres|air_temp|wind_dir|wind_speed|salinity|pre_value|real_value|\n",
      "+-------------------+--------+--------+--------+----------+--------+---------+----------+\n",
      "|2022-03-01 00:00:00|  1016.1|    10.9|   278.0|       0.3|    32.6|      134|       160|\n",
      "|2022-03-01 01:00:00|  1015.6|    11.2|   117.0|       1.7|    32.7|       92|       121|\n",
      "|2022-03-01 02:00:00|  1014.6|    12.2|   104.0|       1.7|    32.6|       53|        81|\n",
      "|2022-03-01 03:00:00|  1013.5|    12.1|   140.0|       0.5|    32.6|       23|        51|\n",
      "|2022-03-01 04:00:00|  1013.0|    12.2|   276.0|       1.3|    32.6|       14|        39|\n",
      "+-------------------+--------+--------+--------+----------+--------+---------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "output_df = output_df.sort(asc(\"record_time\"))\n",
    "output_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9546bfd6-dc35-48b9-a1d3-15ec0809def1",
   "metadata": {},
   "source": [
    "#### DB Table 에서 해당 데이터가 어디 관측소에서 수집 됐는지 알기 위한 value 추가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b7e5db0d-cde3-4e75-ac97-0fd3698af7d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_df = output_df.withColumn('id',lit(response[\"result\"][\"meta\"][\"obs_post_id\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e847c69f-80c4-4849-a476-d9832ff0014b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------+--------+--------+----------+--------+---------+----------+-------+\n",
      "|        record_time|air_pres|air_temp|wind_dir|wind_speed|salinity|pre_value|real_value|     id|\n",
      "+-------------------+--------+--------+--------+----------+--------+---------+----------+-------+\n",
      "|2022-03-01 00:00:00|  1016.1|    10.9|   278.0|       0.3|    32.6|      134|       160|DT_0004|\n",
      "|2022-03-01 01:00:00|  1015.6|    11.2|   117.0|       1.7|    32.7|       92|       121|DT_0004|\n",
      "|2022-03-01 02:00:00|  1014.6|    12.2|   104.0|       1.7|    32.6|       53|        81|DT_0004|\n",
      "|2022-03-01 03:00:00|  1013.5|    12.1|   140.0|       0.5|    32.6|       23|        51|DT_0004|\n",
      "|2022-03-01 04:00:00|  1013.0|    12.2|   276.0|       1.3|    32.6|       14|        39|DT_0004|\n",
      "+-------------------+--------+--------+--------+----------+--------+---------+----------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "output_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0da95593-d076-4c8f-bb76-d9058e8835d6",
   "metadata": {},
   "source": [
    "#### DB data insert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f5dd7682-e1e4-4b53-b059-b200e90c1536",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_df.write.format('jdbc').options(\n",
    "    url='jdbc:mysql://'+ip+'/weather',\n",
    "    driver='com.mysql.jdbc.Driver',\n",
    "    dbtable='DT_0004',\n",
    "    user='mdgome',\n",
    "    password='Rlawjdals1!').mode('append').save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5992edb1-7dc5-4577-8cba-2cbaed6b4da7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df4.write.format('jdbc').options(\n",
    "    url='jdbc:mysql://'+ip+'/weather',\n",
    "    driver='com.mysql.jdbc.Driver',\n",
    "    dbtable='DT_0004_tideCurPre',\n",
    "    user='mdgome',\n",
    "    password='Rlawjdals1!').mode('append').save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0e99fa4a-136f-48f1-9055-57a2b70faa0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d12629db-83fc-49a0-8e64-5d056921f359",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pipeline",
   "language": "python",
   "name": "pipeline"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
