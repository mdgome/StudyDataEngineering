{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d0559ffd-0014-4cf5-83fc-07c3409e56b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "51204697-a372-4bc9-922b-9e0ce24ed3df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import regexp_extract\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "032e2017-9c45-46cc-8afd-b69154b99287",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/02/06 22:36:08 WARN Utils: Your hostname, Jungminui-MacBookPro.local resolves to a loopback address: 127.0.0.1; using 172.30.1.43 instead (on interface en0)\n",
      "22/02/06 22:36:08 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "22/02/06 22:36:09 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .master('local') \\\n",
    "        .appName('airflow log test') \\\n",
    "        .getOrCreate()\n",
    " # spark = SparkSession.builder.master('local').appName(\"airflow log test\").getOrCreate() # change master"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9f52529b-d966-4ddd-b2e7-3c398a31e9ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('spark.master', 'local'),\n",
      " ('spark.driver.port', '50861'),\n",
      " ('spark.sql.warehouse.dir',\n",
      "  'file:/Users/mdgome/workspace/study/StudyDataEngineering/StudyDataEngineering/miniporject%232/code/spark-warehouse'),\n",
      " ('spark.app.name', 'airflow log test'),\n",
      " ('spark.app.startTime', '1644154569044'),\n",
      " ('spark.rdd.compress', 'True'),\n",
      " ('spark.serializer.objectStreamReset', '100'),\n",
      " ('spark.driver.host', '172.30.1.43'),\n",
      " ('spark.submit.pyFiles', ''),\n",
      " ('spark.executor.id', 'driver'),\n",
      " ('spark.submit.deployMode', 'client'),\n",
      " ('spark.app.id', 'local-1644154571110'),\n",
      " ('spark.ui.showConsoleProgress', 'true')]\n"
     ]
    }
   ],
   "source": [
    "import pprint\n",
    "pprint.pprint(spark.sparkContext.getConf().getAll())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "187ed511-e057-4419-97f7-97d1dcc251c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/Users/mdgome/workspace/study/StudyDataEngineering/StudyDataEngineering/miniporject#2/code/MySQL_to_CSV.text\"\n",
    "# rdd = spark.sparkContext.textFile(path)\n",
    "data = spark.read.text(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a0741303-562a-4397-8493-3e1e2b77a35d",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+--------------------+\n",
      "|            datetime|log_level|             message|\n",
      "+--------------------+---------+--------------------+\n",
      "|2022-01-19 11:46:...|     INFO|Started process (...|\n",
      "|2022-01-19 11:46:...|     INFO|Processing file /...|\n",
      "|2022-01-19 11:46:...|     INFO|Filling up the Da...|\n",
      "|2022-01-19 11:46:...|     INFO|DAG(s) dict_keys(...|\n",
      "|                    |         |                    |\n",
      "|                    |         |                    |\n",
      "|2022-01-19 11:46:...|     INFO|Created Permissio...|\n",
      "|2022-01-19 11:46:...|     INFO|Created Permissio...|\n",
      "|2022-01-19 11:46:...|     INFO|         Sync 1 DAGs|\n",
      "|2022-01-19 11:46:...|     INFO|Creating ORM DAG ...|\n",
      "|2022-01-19 11:46:...|     INFO|Setting next_dagr...|\n",
      "|2022-01-19 11:46:...|     INFO|Processing /opt/a...|\n",
      "|2022-01-19 11:46:...|     INFO|Started process (...|\n",
      "|2022-01-19 11:46:...|     INFO|Processing file /...|\n",
      "|2022-01-19 11:46:...|     INFO|Filling up the Da...|\n",
      "|2022-01-19 11:47:...|     INFO|DAG(s) dict_keys(...|\n",
      "|2022-01-19 11:47:...|     INFO|         Sync 1 DAGs|\n",
      "|2022-01-19 11:47:...|     INFO|Setting next_dagr...|\n",
      "|2022-01-19 11:47:...|     INFO|Processing /opt/a...|\n",
      "|2022-01-19 11:47:...|     INFO|Started process (...|\n",
      "+--------------------+---------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "(2209, 3)\n"
     ]
    }
   ],
   "source": [
    "regex = r\"\\[((\\d+-\\d+-\\d+) (\\d+:\\d+:\\d+,\\d+))\\] \\{\\S+\\} (DEBUG|INFO|WARN|FATAL|ERROR|TRACE) - ([a-zA-Z0-9]+.*)\"\n",
    "\n",
    "logs_df = data.select(regexp_extract('value', regex, 1).alias('datetime'),\n",
    "                       regexp_extract('value', regex, 4).alias('log_level'),\n",
    "                      regexp_extract('value', regex,5).alias('message')\n",
    "                     )\n",
    "logs_df.show(truncate=True)\n",
    "print((logs_df.count(), len(logs_df.columns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1be8c100-a2a5-4d25-9751-7ec49132a197",
   "metadata": {},
   "outputs": [],
   "source": [
    "logs_df = logs_df.withColumn('datetime',year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "9b9d5753-f53f-4f54-b3fe-d4c6c3d46460",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'datetime': '2022-01-19 11:46:20,775',\n",
       "  'log_level': 'INFO',\n",
       "  'message': 'Started process (PID=946) to work on /opt/airflow/dags/MySQL_to_CSV.py'},\n",
       " {'datetime': '2022-01-19 11:46:20,780',\n",
       "  'log_level': 'INFO',\n",
       "  'message': 'Processing file /opt/airflow/dags/MySQL_to_CSV.py for tasks to queue'},\n",
       " {'datetime': '2022-01-19 11:46:20,782',\n",
       "  'log_level': 'INFO',\n",
       "  'message': 'Filling up the DagBag from /opt/airflow/dags/MySQL_to_CSV.py'},\n",
       " {'datetime': '2022-01-19 11:46:22,634',\n",
       "  'log_level': 'INFO',\n",
       "  'message': \"DAG(s) dict_keys(['second_assingnment']) retrieved from /opt/airflow/dags/MySQL_to_CSV.py\"},\n",
       " {'datetime': '', 'log_level': '', 'message': ''}]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json_bulk = logs_df.toJSON().map(lambda x: json.loads(x)).collect()\n",
    "json_bulk[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fe837b2-8272-41b4-a1d9-99b1f93d15a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(json_bulk)\n",
    "type(logs_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "efc3e25e-a1d1-4701-991e-47406d5f7121",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'iterrows'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [66]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfilterKeys\u001b[39m(document):\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {key: document[key] \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m use_these_keys }    \n\u001b[0;32m---> 22\u001b[0m \u001b[43mhelpers\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbulk\u001b[49m\u001b[43m(\u001b[49m\u001b[43mes_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdoc_generator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogs_df\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/pipeline/lib/python3.9/site-packages/elasticsearch/helpers/actions.py:410\u001b[0m, in \u001b[0;36mbulk\u001b[0;34m(client, actions, stats_only, ignore_status, *args, **kwargs)\u001b[0m\n\u001b[1;32m    408\u001b[0m \u001b[38;5;66;03m# make streaming_bulk yield successful results so we can count them\u001b[39;00m\n\u001b[1;32m    409\u001b[0m kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myield_ok\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 410\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m ok, item \u001b[38;5;129;01min\u001b[39;00m streaming_bulk(\n\u001b[1;32m    411\u001b[0m     client, actions, ignore_status\u001b[38;5;241m=\u001b[39mignore_status, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    412\u001b[0m ):\n\u001b[1;32m    413\u001b[0m     \u001b[38;5;66;03m# go through request-response pairs and detect failures\u001b[39;00m\n\u001b[1;32m    414\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ok:\n\u001b[1;32m    415\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m stats_only:\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/pipeline/lib/python3.9/site-packages/elasticsearch/helpers/actions.py:319\u001b[0m, in \u001b[0;36mstreaming_bulk\u001b[0;34m(client, actions, chunk_size, max_chunk_bytes, raise_on_error, expand_action_callback, raise_on_exception, max_retries, initial_backoff, max_backoff, yield_ok, ignore_status, *args, **kwargs)\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    285\u001b[0m \u001b[38;5;124;03mStreaming bulk consumes actions from the iterable passed in and yields\u001b[39;00m\n\u001b[1;32m    286\u001b[0m \u001b[38;5;124;03mresults per action. For non-streaming usecases use\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    315\u001b[0m \u001b[38;5;124;03m:arg ignore_status: list of HTTP status code that you want to ignore\u001b[39;00m\n\u001b[1;32m    316\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    317\u001b[0m actions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmap\u001b[39m(expand_action_callback, actions)\n\u001b[0;32m--> 319\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m bulk_data, bulk_actions \u001b[38;5;129;01min\u001b[39;00m _chunk_actions(\n\u001b[1;32m    320\u001b[0m     actions, chunk_size, max_chunk_bytes, client\u001b[38;5;241m.\u001b[39mtransport\u001b[38;5;241m.\u001b[39mserializer\n\u001b[1;32m    321\u001b[0m ):\n\u001b[1;32m    323\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m attempt \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(max_retries \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m    324\u001b[0m         to_retry, to_retry_data \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/pipeline/lib/python3.9/site-packages/elasticsearch/helpers/actions.py:155\u001b[0m, in \u001b[0;36m_chunk_actions\u001b[0;34m(actions, chunk_size, max_chunk_bytes, serializer)\u001b[0m\n\u001b[1;32m    148\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;124;03mSplit actions into chunks by number or size, serialize them into strings in\u001b[39;00m\n\u001b[1;32m    150\u001b[0m \u001b[38;5;124;03mthe process.\u001b[39;00m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    152\u001b[0m chunker \u001b[38;5;241m=\u001b[39m _ActionChunker(\n\u001b[1;32m    153\u001b[0m     chunk_size\u001b[38;5;241m=\u001b[39mchunk_size, max_chunk_bytes\u001b[38;5;241m=\u001b[39mmax_chunk_bytes, serializer\u001b[38;5;241m=\u001b[39mserializer\n\u001b[1;32m    154\u001b[0m )\n\u001b[0;32m--> 155\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m action, data \u001b[38;5;129;01min\u001b[39;00m actions:\n\u001b[1;32m    156\u001b[0m     ret \u001b[38;5;241m=\u001b[39m chunker\u001b[38;5;241m.\u001b[39mfeed(action, data)\n\u001b[1;32m    157\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ret:\n",
      "Input \u001b[0;32mIn [66]\u001b[0m, in \u001b[0;36mdoc_generator\u001b[0;34m(df)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdoc_generator\u001b[39m(df):\n\u001b[0;32m----> 6\u001b[0m     df_iter \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miterrows\u001b[49m()\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m index, document \u001b[38;5;129;01min\u001b[39;00m df_iter:\n\u001b[1;32m      8\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m {\n\u001b[1;32m      9\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_index\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest_airflow_log\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     10\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_type\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_doc\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     11\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_id\u001b[39m\u001b[38;5;124m\"\u001b[39m : \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdocument[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     12\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_source\u001b[39m\u001b[38;5;124m\"\u001b[39m: filterKeys(document),\n\u001b[1;32m     13\u001b[0m             } \n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/pipeline/lib/python3.9/site-packages/pyspark/sql/dataframe.py:1659\u001b[0m, in \u001b[0;36mDataFrame.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1649\u001b[0m \u001b[38;5;124;03m\"\"\"Returns the :class:`Column` denoted by ``name``.\u001b[39;00m\n\u001b[1;32m   1650\u001b[0m \n\u001b[1;32m   1651\u001b[0m \u001b[38;5;124;03m.. versionadded:: 1.3.0\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1656\u001b[0m \u001b[38;5;124;03m[Row(age=2), Row(age=5)]\u001b[39;00m\n\u001b[1;32m   1657\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1658\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns:\n\u001b[0;32m-> 1659\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[1;32m   1660\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, name))\n\u001b[1;32m   1661\u001b[0m jc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jdf\u001b[38;5;241m.\u001b[39mapply(name)\n\u001b[1;32m   1662\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m Column(jc)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'iterrows'"
     ]
    }
   ],
   "source": [
    "from elasticsearch import Elasticsearch\n",
    "from elasticsearch import helpers\n",
    "\n",
    "es_client = Elasticsearch(hosts=['localhost:9200'],http_compress=True)\n",
    "def doc_generator(df):\n",
    "    df_iter = df.iterrows()\n",
    "    for index, document in df_iter:\n",
    "        yield {\n",
    "                \"_index\": 'test_airflow_log',\n",
    "                \"_type\": \"_doc\",\n",
    "                \"_id\" : f\"{document['id']}\",\n",
    "                \"_source\": filterKeys(document),\n",
    "            } \n",
    "    raise StopIteration\n",
    "\n",
    "use_these_keys = ['datetime', 'log_level', 'message']\n",
    "\n",
    "def filterKeys(document):\n",
    "    return {key: document[key] for key in use_these_keys }    \n",
    "\n",
    "\n",
    "helpers.bulk(es_client, doc_generator(logs_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "16412053-b715-4e0d-9018-553c837be0ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "5af6cbb1-f427-47ce-839e-da1d40276c80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/mdgome/workspace/study/StudyDataEngineering/StudyDataEngineering/miniporject#2/code\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5af5b640-64ea-42fc-8129-8a48e5a49b22",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pipeline",
   "language": "python",
   "name": "pipeline"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
